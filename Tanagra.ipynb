{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kET2Wqea6Trf"
      },
      "source": [
        "### Tanagra (new version)\n",
        "## By Motasem ALRAHABI, ObTIC  Sorbonne Université, juin 2023\n",
        "## Linkt: https://github.com/obtic-sorbonne/Tanagra2\n",
        "\n",
        "This script do the following:\n",
        "1.   Each text is segmented, sentence by sentence (with spacy).\n",
        "2.   Each sentence is read and searched for all LOC named entities (with spacy).\n",
        "3.   For each entity it finds the geographical coordinates (with geopy / geonames).\n",
        "4.   If at least one named entity is found in the sentence, a function analyzes the positive, negative and neutral sentiments on the sentence level (using spacy).\n",
        "5.   It Displays on a map the named entities found (with folium).\n",
        "6.   The color of each icon is calculated according to the average sentiment of this entity in the whole corpus: green for positive, red for negative and gray for neutral.\n",
        "7.   The size of each icon on the map is proportional to the number of occurrences of this entity in the whole corpus.\n",
        "8.   For each icon, a popup shows the place name, the occurences number, the number of positive, negative and neutral sentiments in the whole corpus.\n",
        "9.   I save each entity in an output csv file: file, sentence, sentiment, lat, long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Ll-uwX1QqU"
      },
      "outputs": [],
      "source": [
        "! pip install spacy geopy folium textblob transformers\n",
        "! pip install geonamescache\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from textblob import TextBlob\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "import geonamescache\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49inDm5O590b"
      },
      "outputs": [],
      "source": [
        "# Charger le modèle français de spaCy\n",
        "! python -m spacy download fr_core_news_sm\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# Charger la bdd geonames:\n",
        "gc = geonamescache.GeonamesCache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdZ0DDNqF9oS"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n",
        "from transformers import pipeline\n",
        "#sentiment_analysis_model = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", framework=\"tf\")\n",
        "sentiment_analysis_model = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", framework=\"pt\")\n",
        "\n",
        "\n",
        "# Essayer aussi https://huggingface.co/philschmid/pt-tblard-tf-allocine\n",
        "# Essayer aussi https://huggingface.co/moussaKam/barthez\n",
        "# Essayer aussi https://pypi.org/project/aspect-based-sentiment-analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxrm2QJM-Zwh"
      },
      "outputs": [],
      "source": [
        "# Segment each text into sentences (in the whole corpus)\n",
        "def segment_sentences(directory_path):\n",
        "    all_sentences = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                doc = nlp(text)\n",
        "                sentences = [sent.text for sent in doc.sents]\n",
        "                all_sentences.extend(sentences)\n",
        "    print(all_sentences)\n",
        "    return all_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIVKs_lN9qFe"
      },
      "outputs": [],
      "source": [
        "# Analyzing each sentence:\n",
        "\n",
        "blacklist = [\"l’\", \"qu’\", \"m’\", \"t’\", \"s’\", \"L’\"]\n",
        "\n",
        "#with open(\"blacklist.txt\", \"r\", encoding='utf-8') as file:\n",
        "#  blacklist = [word.strip() for word in file.readlines()]\n",
        "\n",
        "def analyze_sentence(sentence):\n",
        "  geolocator = Nominatim(user_agent='my-app', timeout=10)\n",
        "  doc = nlp(sentence)\n",
        "  analyzed_sentence = []\n",
        "  entities = []\n",
        "  latitude = None\n",
        "  longitude = None\n",
        "  sentiment_score_sentence = 0\n",
        "  sentiment_label_sentence = \"\"\n",
        "\n",
        "  for ent in doc.ents:\n",
        "      if ent.label_ == \"LOC\":\n",
        "        if ent.text not in blacklist:\n",
        "            entities.append(ent.text)\n",
        "\n",
        "            # Geopy library will not be able to connect to the Nominatim server in order to geocode all locations, due to the connection timing out.\n",
        "            # We can handle this issue in a few ways: Increase the timeout, Retry the request (try/except...) or use a DB like Geonames ou OpenStreetMap\n",
        "            # pour le moment j'utilse un compteur à personnaliser \"counter\", voir plus loin.\n",
        "            # Je vais utiliser geonamescache (geonames.db est plus complet mais plus lourd), et si l'info manque, j'appelle l'api geonames en ligne.\n",
        "\n",
        "\n",
        "            # Try using Geonamescache as primary source\n",
        "            matching_cities = gc.get_cities_by_name(ent.text)\n",
        "            #essayer aussi: get_cities(), get_countries(), get_countries_by_names(), get_continents(), etc.\n",
        "            if matching_cities:\n",
        "                first_matching_city = list(matching_cities[0].values())[0]\n",
        "                latitude = first_matching_city['latitude']\n",
        "                longitude = first_matching_city['longitude']\n",
        "            else:\n",
        "                # Use Nominatim as fallback\n",
        "                try:\n",
        "                    location = geolocator.geocode(ent.text)\n",
        "                    if location:\n",
        "                        latitude = location.latitude\n",
        "                        longitude = location.longitude\n",
        "                except GeocoderTimedOut:\n",
        "                    location = None\n",
        "\n",
        "            if entities and latitude and longitude:\n",
        "              sentiment_score_sentence = sentiment_analysis_model(sentence)[0][\"score\"]\n",
        "              sentiment_label_sentence = sentiment_analysis_model(sentence)[0]['label']\n",
        "            result = {\n",
        "                \"entity\": ent.text,\n",
        "                \"sentiment_label_sentence\": sentiment_label_sentence,\n",
        "                \"sentiment_score_sentence\": sentiment_score_sentence,\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude\n",
        "            }\n",
        "            analyzed_sentence.append(result)\n",
        "\n",
        "  return analyzed_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nE48twH9fJq"
      },
      "outputs": [],
      "source": [
        "# Texts\n",
        "\n",
        "# Atention: le sentiment général est celui de la phrase dans son ensemble, et non pas des propositions autour des lieux !!!\n",
        "# Pour le moment le sentiment est celui de l'ensemble des données en entrées:\n",
        "    # si un seul text --> sentiment au niveau de ce texte.\n",
        "    # si folder --> sentiment au niveau du folder\n",
        "\n",
        "# on peut affiner plus tard pour traiter les sentiments de chaque fichier à part dans un folder !\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "all_sentences = segment_sentences(\"/content/drive/MyDrive/Colab_Notebooks/input/\")\n",
        "\n",
        "#uploaded = files.upload()\n",
        "#filename = next(iter(uploaded.keys()))\n",
        "#all_sentences = uploaded[filename].decode('utf-8')\n",
        "#all_sentences = [\"J'adore Marseille car Marseille est une belle ville. Je déteste Lyon. J'aime Lille\"]\n",
        "#all_sentences = segment_sentences(\"./input/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2dDxL1sMq-W"
      },
      "outputs": [],
      "source": [
        "# Traitement:\n",
        "analyzed_sentences = []\n",
        "# Save entity information to a CSV file\n",
        "with open('output.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=\"\\t\")\n",
        "    writer.writerow(['file', 'sentence', 'entity', 'label', 'score', 'lat', 'long'])\n",
        "    counter = 0\n",
        "    # Analyze each sentence\n",
        "    for sentence in all_sentences:\n",
        "      if counter>=1000: # le nombre maximum supporté par geonames\n",
        "        break\n",
        "      analyzed_sentence = analyze_sentence(sentence)\n",
        "      for result in analyzed_sentence:\n",
        "        entity = result[\"entity\"]\n",
        "        sentiment_label = result[\"sentiment_label_sentence\"]\n",
        "        sentiment_score = result[\"sentiment_score_sentence\"]\n",
        "        latitude = result[\"latitude\"]\n",
        "        longitude = result[\"longitude\"]\n",
        "\n",
        "        # check if latitude and longitude are None\n",
        "        if latitude is None or longitude is None:\n",
        "            print(f\"Skipping sentence due to missing location data: {sentence}\")\n",
        "            continue  # skip this sentence and move to the next one\n",
        "\n",
        "\n",
        "\n",
        "        writer.writerow([\"-->\", sentence, entity, sentiment_label, round(sentiment_score, 2), latitude, longitude])\n",
        "        counter += 1\n",
        "        print(f\"Sentiment de la phrase {counter}: {sentence[:20]}... ; entité:[{entity}] ; polarity: {sentiment_label} ; score: {round(sentiment_score, 2)}\")\n",
        "        analyzed_sentences.append({\n",
        "            'sentence': sentence,\n",
        "            'entity': entity,\n",
        "            'sentiment_label': sentiment_label,\n",
        "            'sentiment_score': round(sentiment_score, 2),\n",
        "            'latitude': latitude,\n",
        "            'longitude': longitude\n",
        "        })\n",
        "\n",
        "# Iterate analyzed_sentences and create a new dictionary\n",
        "entities_dict = {}\n",
        "for sentence_info in analyzed_sentences:\n",
        "    entity = sentence_info['entity']\n",
        "    sentiment_label = sentence_info['sentiment_label']\n",
        "\n",
        "    # Initialize entity information if it doesn't exist\n",
        "    if entity not in entities_dict:\n",
        "        entities_dict[entity] = {\n",
        "            'latitude': sentence_info['latitude'],\n",
        "            'longitude': sentence_info['longitude'],\n",
        "            'occurrences': 1,\n",
        "            'positive_labels': 0,\n",
        "            'negative_labels': 0,\n",
        "            'neutral_labels': 0,\n",
        "            'overall_sentiment': sentiment_label\n",
        "        }\n",
        "    else:\n",
        "        entities_dict[entity]['occurrences'] = entities_dict[entity]['occurrences'] + 1\n",
        "\n",
        "    # Update sentiment label counts and overall sentiment\n",
        "    if sentiment_label in ['4 stars', '5 stars']:\n",
        "        entities_dict[entity]['positive_labels'] += 1\n",
        "    elif sentiment_label in ['1 star', '2 stars']:\n",
        "        entities_dict[entity]['negative_labels'] += 1\n",
        "    elif sentiment_label == '3 stars':\n",
        "        entities_dict[entity]['neutral_labels'] += 1\n",
        "\n",
        "    # Update overall sentiment based on counts\n",
        "    for entity in entities_dict:\n",
        "      positive_count = entities_dict[entity]['positive_labels']\n",
        "      negative_count = entities_dict[entity]['negative_labels']\n",
        "      neutral_count = entities_dict[entity]['neutral_labels']\n",
        "\n",
        "      if positive_count > negative_count and positive_count > neutral_count:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Positive'\n",
        "      elif negative_count > positive_count and negative_count > neutral_count:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Negative'\n",
        "      elif neutral_count > positive_count and neutral_count > negative_count:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Neutral'\n",
        "      else:\n",
        "        entities_dict[entity]['overall_sentiment'] = 'Mixed'\n",
        "\n",
        "print(entities_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUmP18ey7xTh"
      },
      "outputs": [],
      "source": [
        "# Visualisation\n",
        "\n",
        "# Initialize a map\n",
        "map_center = [0, 0]  #[48.8566, 2.3522]\n",
        "map_zoom = 2.5  # Adjust the zoom level as needed\n",
        "map = folium.Map(location=map_center, zoom_start=map_zoom)\n",
        "\n",
        "# Iterate over entities in entities_dict\n",
        "for entity, info in entities_dict.items():\n",
        "  #print(\"entity\", entity)\n",
        "  latitude = info['latitude']\n",
        "  longitude = info['longitude']\n",
        "  occurrences = info['occurrences']\n",
        "  overall_sentiment = info['overall_sentiment']\n",
        "  positive_labels = info['positive_labels']\n",
        "  negative_labels = info['negative_labels']\n",
        "  neutral_labels = info['neutral_labels']\n",
        "\n",
        "  # Determine icon color based on overall sentiment\n",
        "  if overall_sentiment == 'Positive':\n",
        "    color = 'green'\n",
        "  elif overall_sentiment == 'Negative':\n",
        "    color = 'red'\n",
        "  elif overall_sentiment == 'Neutral':\n",
        "    color = 'gray'\n",
        "  elif overall_sentiment == 'Mixed':\n",
        "    color = 'blueviolet'\n",
        "  else:\n",
        "    color = 'blue'\n",
        "  #print(overall_sentiment,\"  \", color)\n",
        "\n",
        "\n",
        "  scaling_factor = 1  # Adjust this value to control the marker size scaling\n",
        "  size = int(occurrences) * scaling_factor\n",
        "\n",
        "\n",
        "\n",
        "  # Create a popup with entity information\n",
        "  popup = f\"<b>Entity:</b> {entity}<br>\"\n",
        "  popup += f\"<b>Occurrences:</b> {occurrences}<br>\"\n",
        "  popup += f\"<b>Sentiment:</b> {overall_sentiment}<br>\"\n",
        "  popup += f\"<b>Positive:</b> {positive_labels}<br>\"\n",
        "  popup += f\"<b>Negative:</b> {negative_labels}<br>\"\n",
        "  popup += f\"<b>Neutral:</b> {neutral_labels}<br>\"\n",
        "\n",
        "  folium.CircleMarker(\n",
        "      location=[latitude, longitude],\n",
        "      radius=size,\n",
        "      popup=popup,\n",
        "      color=color,\n",
        "      fill=True,\n",
        "      fill_color=color,\n",
        "      #tooltip=f\"Entity: {entity}\"\n",
        "      tooltip=entity\n",
        "  ).add_to(map)\n",
        "\n",
        "#map.save(\"map.html\")\n",
        "map"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
